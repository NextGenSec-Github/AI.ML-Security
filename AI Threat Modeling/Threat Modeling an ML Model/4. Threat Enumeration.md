# Threat Enumeration
In this section, we enumerate potential threats to the machine learning (ML) model, considering the assets identified in the previous section. Understanding these threats helps in identifying vulnerabilities and assessing the risks associated with the ML system.


## 1. Data Poisoning:

- **Threat:** Malicious actors may inject poisoned data into the training dataset to manipulate the model's behavior.
- **Impact:** Data poisoning can lead to biased model outcomes, reduced accuracy, or compromised model integrity.
- **Likelihood:** Moderate to high, especially in collaborative or open data environments where data quality and provenance may not be well-controlled.
- **Mitigation:** Implement data validation and sanitization techniques to detect and remove anomalous or malicious data points. Employ data provenance tracking and model robustness testing to identify and mitigate data poisoning attacks.


## 2. Adversarial Attacks:

- **Threat:** Adversaries may craft adversarial examples to deceive the ML model into making incorrect predictions.
- **Impact:** Adversarial attacks can lead to misclassification, evasion of detection mechanisms, or exploitation of vulnerabilities in the model's decision boundary.
- **Likelihood:** Moderate to high, as adversarial attacks are well-documented and increasingly prevalent in ML systems.
- **Mitigation:** Apply adversarial robustness techniques such as adversarial training, input preprocessing, and model regularization to enhance the model's resilience against adversarial attacks. Employ defensive mechanisms such as input perturbation or ensemble methods to detect and mitigate adversarial examples during inference.


